Arrays: An array is a data structure that holds multiple elements. It is given a specified amount of space in memory where all elements live one be side the other. Because arrays are only assigned a specific amount of memory, they cannot be altered in size. While arrays in JavaScript can hold multiple different data types, most other languages only allow one assigned data type in an array.

Arraylists: An array list is a data structure that relies on the array data structure. It's primary function is the expand the features of an array to allow for them to expand. It does this by simply creating a new array and copying all the elements of the old array into the new one. This is done on insertion and deletion of an element. JavaScript arrays are, by design, array lists out of the box.

Linked List: A linked list is a data structure that houses multiple elements in memory. It is similar in function to an array but shares two important distinctions. First, linked list nodes do not need to be located directly beside each other in memory and in fact linked list nodes have memory dynamically allocated to them. Because of this, a linked list node will have no knowledge of other nodes. To handle this, each node points to the others address in memory and the list is managed by traversing through the nodes. The second distinction is they are not generally found pre-written in languages and must be written by a developer to be utilized.

Graph: A data type that holds multiple elements and relationships to each element. The elements themselves are called nodes and each relationship to other nodes are called edges. A graph can be bi-directional or uni-directional which means you can traverse from each node to the other or only traverse from a source node to a destination node respectively. While this does not necessarily describe graphs, they can be cyclical which means that a collection of nodes will allow traversal continuously amongst nodes without crossing the same edge back-to-back. Graphs can be saved in 3 ways:

Edge list: A list of all the edges in the graph. This does not describe the amount of nodes in the graph.

Adjacency Matrix: A matrix of n x n nodes which shows relationships between each other. This matrix is generally saved in a two dimensional array. A undirected graph (has bi-directional edges) will have mirrored data across the diagonal of the graph. This method is the most efficient way of saving graphs in terms of time.

Adjacency List: A list of nodes and the edges that they connect to. Adjacency lists can be made up of other data structures that will house the list of edges for a node. This method is the most efficient method of saving graphs in terms of space.

Tree: A type of graph. Trees have specific attributes that distinguish them from normal graphs. They always start from a root node. For every node, they have n - 1 edges. A tree cannot be cyclical. The most widely used type of tree is the binary tree. This has the added attribute of each node only having up to two children. This isn't strictly the only type of tree but is the most widely used and is the foundation of other data structures.

Trie: A type of tree. A trie mainly functions as a way to identify words or sentences by relating a letter or word to one another. This allows for look ups of words or sentences from input quickly.

Heaps: A type of tree. Heaps come in two types, min heap and max heap. Each type will sort elements to ensure that the max or min of the entire collection is at the top of the heap. Heaps do this by performing a heapify function on every insertion. An element is added to the end of the collection and is compared with its parent. Depending on the type of heap, if the parent is smaller or larger, the element is swapped with the parent. This goes on until the element either does not meet the comparison requirements or it becomes the root of the tree. Heaps are generally implemented as arrays or arraylists because of their quick look up time for root nodes and end nodes.

Hash Tables: A data structure that allows for quick look ups through hashing. Hashing is the act of taking an input, running that input through a function and getting a result that will become an index in the hash table with the index always being within the table's size. Hash tables are generally written as arrays to ensure fast look up times. There are some problems that become immediate with hashing. Two different input can hash to the same index value and cannot take up the same space. This is called a collision and dealt with the following two ways:

Seperate Chaining: Implement an auxillary data structure to save data where the input causes a collision. General choices for auxillary data structures are linked lists (because their size is dynamic), arrays, another hash table.

Open addressing: Whenever a collision is detected, the collided element will be moved to the next index. If there is another element there, the element is moved down again and will continue to do so until an empty slot can be found.

Binary Search: An algorithm for used for searching through a sorted data structure. It does so by checking the middle element and if the middle element is not the target element, the checking the element is smaller or larger than the target element and checking either the right or left half of the data structure. It will do this, cutting the search space in half each time until the element is found or the search space is 0. It is important to note that the structure of this element is the basis of the binary search tree.

Depth-First Search: An algorithm for searching through trees or graphs. It does so by traversing down a path until it hits a visited node (graph) or a leaf node(tree). It is important to note that there are three types of depth-first search implementations to traverse trees:

Pre-order: Print the root node's value then traverse left node and then right node.

Post-order: Traverse both left and right nodes then print the root node's value.

In-Order: Traverse the left node, print the root node's value and then traverse the right now. This traversal will print the node in sorted order.

Breadth-First Search: An algorithm for searching through trees or graphs. It traverses by printing each node, level by level until the last root node. It will do the same for graphs, only terminating like in depth-first search when a node it encounters is marked as visited.

Merge Sort: An algorithm to sort elements in a data structure. It does so by breaking the data structure into halves and then breaking those halves into halves until there is only one or no elements in each half. It will then add the element (considered sorted) into an array and return it. As it merges elements, it will place the smallest or largest element into the array first and move forward until all elements are places in the array. It will continue to do this until all elements are placed in the array in sorted order.

Quick Sort: An algorithm for sorting elements in a data structure. It does so by selecting a pivot value, moving it out of the way then scanning through the data structure for all elements that are less than the pivot value and moving them to the left of the pivot value. Once the entire data structure is scanned, the pivot is placed where it belongs and the process repeats with each section of the array that now appears on either side of the pivot. This process continues until each pivot position is placed in its correct location in the array.

Bit Manipulation: The process of performing tasks on specific bits. This utilizes bitwise operators that allow the use of logical operations and performs them to numbers and elements on a binary level. There are also operations called bit shifting that moves bits in a chosen direction. This work requires deep knowledge of binary numbers as well as how the computer stores and utilizes numbers as well as heavy math fundamentals.

Dynamic Programming: The act of breaking a large problem into smaller sub problems with the hope that the combination of the smaller sub problems will combine to answer the larger problem. Sub-problems are usually accomplished in the same way as the larger main problem but with a smaller input space. A key note of dynamic programming is that solutions to sub-problems are saved so that they do not need to be re-calculated and thus saving on processing time. There are two methods of breaking problems down into sub-problems:

Top-Down approach: Take the sub problem and then breaking it into smaller problems, going further and further until you reach a base sub-problem. Once that is solved, you use that solution to solve other sub problems. Merge Sort and Quick Sort are examples of this methodology.

Bottom-up approach: Analyzing a problem and then identifying the smallest problem that can be solved first before moving forward. With the solution to the base problem, you continue to solve other sub problems until you finally reach the solution to the inital main problem.

Recursion: The act of calling a function within itself. A technique from Mathematics, functions called recursively will complete first before allowing the function that called it to complete. Recursive functions are usually called with different parameters (usually a smaller input space) to do work that the calling function will then use. Recursed functions that also call themselves will have no way of knowing when to end the recursion and so will need a base condition to end the recursion when parameters are met. This is an important part of recursion which stops programs from running infinitely or until memory is used up.

Memory (Call Stack): The call stack is a place in memory that is allocated for programs to store functions that are called, their variables and space for them to return the result. The call stack is allocated depending on the program and cannot be changed when a program is running. It is possible to fill the call stack and cause a stack overflow error when a function is called and there is no longer space on the call stack to place it there. Functions will live out the entirity of the operation before being removed from the call stack and is only removed upon completion. This makes recursion a common cause of stack overflow errors in software.

Memory (Heap): The heap is a place in memory reserved for the programmer to place information that they may need later on in the programs execution. The heap space is the size of available memory in the system and in older programming languages are unpoliced. Garbage memory is memory (or parts of memory) that has information from some previous program that wasn't deleted and if read can be potentially anything. This is usually not an issue unless a program tries to read from a place in the heap where data from a differnt application was saved. The data may corrupt the running application and crash it unexpectedly. The most important issue with heap space is memory leakage. When data is saved to the heap, that space in memory becomes reserved for that data. If the memory is unreserved but not cleaned then it can potentially become garbage data to another program. But if a space in the heap is reserved and never un-reserved is called a memory leak. It becomes especially challenging when the address to that space in memory is lost and the program can no longer go to that address to free up memory. Once locked, that address in memory will remain locked for as long as the computing device remains on and is essentially unavailable. If continued unchecked, this will eventually decrease the amount of memory available for running programs to the point where they can no longer allocate space for their needs. Older programming languages that allow this type of issue to occur are known to not be memory-safe. In memory-safe languages, there is some mechanism to de-allocate memory when they are no longer reachable by any running progams. This mechanism is usually called garbage collection and will free memory from the heap whenever a program can no longer reach it. Many newer programming languages implement garbage collection (such as Java, JavaScript, etc.) but older languages such as C and C++ do not have this and are considered memory-unsafe.

Big O Notation: A type of notation to measure aspects of written software. Big O notation is used to measure attributes of an application in a worse case scenario. This is the main focus of software developers because if an application at it's worse case has attrbutes are favorable then the application is considered to be good. The two attributes that Big O notation measures is speed of execution and memory size. The most important aspect of Big O notation and any other Space/Time notation is that they aren't reliant on physical units of measure. Space/Time notation is an abstract measurement that measures the execution of each line of code independent of hardware configuration. This allows Space/Time notations to be hardware agnostic and gives the notation and genearalization that allows developers to analyze code based on their actions as opposed to their actions on specific platforms.